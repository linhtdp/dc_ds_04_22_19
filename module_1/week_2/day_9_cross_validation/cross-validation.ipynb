{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic: Cross Validation in predictive modeling\n",
    "\n",
    "\n",
    "## Problem\n",
    "\n",
    "When you do `train-test-split` you are sampling randomly to identify the train and test datasets.\n",
    "\n",
    "![img](train-test-examp.png)\n",
    "\n",
    "But what about **sampling error**? What if you get a **BAD SAMPLE**?!\n",
    "\n",
    "![bad](https://media.giphy.com/media/cJjQJWU70DSuHzx4oR/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Task\n",
    "![map](map.png) \n",
    "\n",
    "Build a multivariate Ordinary Least Squares regression model to predict \"TARGET_deathRate\"with a more robust model validation method, cross-validation.<br>\n",
    "We have data aggregated from a number of sources including the American Community Survey (census.gov), clinicaltrials.gov, and cancer.gov. Most of the data preparation process can be viewed here.\n",
    "\n",
    "\n",
    "## Learning Goals:\n",
    "\n",
    "- Describe the elements of  K-fold Cross validation \n",
    "- Recognize how K-fold cross validation is superior to normal validation testing\n",
    "- Apply K-fold cross validation to a dataset\n",
    "- Apply K-fold cross validation to Module 1 project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation\n",
    "Let us talk about `training` and `testing.`\n",
    "\n",
    "![train-test](why-train-test.png)\n",
    "\n",
    "We split to prevent:\n",
    "\n",
    "![fit-pit](overfit_underfit.png)\n",
    "(found on [this blog](https://rmartinshort.jimdo.com/2019/02/17/overfitting-bias-variance-and-leaning-curves/)). \n",
    "\n",
    "But what if by random chance of your training dataset split - your training data isn't representative? what if it includes some wacky data?\n",
    "\n",
    "![but what if](bad-split.png)\n",
    "\n",
    "k-fold averages that out, and also keeps from “overfitting” and “underfitting.”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goal 1: Describe the elements of K-fold Cross validation \n",
    "\n",
    "In the context of modeling, K-fold cross validation sits under the Stage 6- Predictive Modeling, in the 7 stage Data Science Lifecycle.\n",
    "\n",
    "![chart](chart.png)\n",
    "\n",
    "K-fold cross validation essentially helps us increase the accuracy of any Machine learning model. It does this by taking the average of the results of training and testing data from given dataset. This in turn is by dividing the dataset into several (“k”) folds. Then, Training data on “k-1” folds and testing on “kth” fold. Repeat this “k” times and average the result.\n",
    "\n",
    "![cross-val](cross-val-graphic.png)\n",
    "(graphic from [here](https://towardsdatascience.com/cross-validation-70289113a072) )\n",
    "\n",
    "We can compare the resultant accuracy by taking the average of accuracy calculated during each of the folds. This tends to give a more real picture of the machine learning model performance. \n",
    "\n",
    "The cross validation technique can be used to compare the performance of different machine learning models on the same data set. To understand this point better, let us consider the following example.\n",
    "\n",
    "Go through [this blog](https://towardsdatascience.com/5-reasons-why-you-should-use-cross-validation-in-your-data-science-project-8163311a1e79) to hit the topic home.\n",
    "\n",
    "## Learning Goal 2:  Explain to Greg\n",
    "![img2](thinking.jpeg)\n",
    "\n",
    "You've hired Greg to build models for you. He's stressed and trying to tell you there isn't enough time to do a cross validation and that one train-test split should be enough.\n",
    "\n",
    "Write down what you would say to Greg and then tell it to your neighbor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goal 3: Applying k-fold cross validation\n",
    "\n",
    "### Try the code in each of these articles:\n",
    "\n",
    "### One half of room:\n",
    "This is a good tech blog:\n",
    "\n",
    "[this blog is a good one](https://machinelearningmastery.com/k-fold-cross-validation/)\n",
    "\n",
    "### Other half of room:\n",
    "[another good example](https://medium.com/datadriveninvestor/k-fold-cross-validation-6b8518070833)\n",
    "\n",
    "\n",
    "### Task: \n",
    "Write the most important  parts of code from each post on the board & then discuss\n",
    "\n",
    "- What did you need to specify?\n",
    "- What new libraries did you use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model to predict cancer\n",
    "\n",
    "\n",
    "```\n",
    "cancer_rates = pd.read_csv('https://query.data.world/s/5ylxfjp6oymzhuhhzwmlbqxzcw6etz')\n",
    "\n",
    "households = pd.read_csv('https://download.data.world/s/3nopgtdm2fwjgidovkostutkfitlps')\n",
    "```\n",
    "\n",
    "[Here is the documentatiopn](https://data.world/exercises/linear-regression-exercise-1/workspace/data-dictionary) for this data.\n",
    "\n",
    "Integrate this new knowledge of k-fold cross validation to build a model and calculate the average performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessment\n",
    "\n",
    "Did they achieve all the learning goals from the start? How do you confirm? You can use many different methods:\n",
    "\n",
    "Review questions? (make into quiz)\n",
    "\n",
    "- What is “training” and “testing” \n",
    "- What is underfitting?\n",
    "- What is overfitting?\n",
    "- What is the data science lifecycle? (students should be able to articulate the 7 steps in the - pie chart above)\n",
    "- What is k-fold cross validation?\n",
    "\n",
    "\n",
    "Why is it useful?\n",
    "\n",
    "### Reflection/Key Takeaways\n",
    "\n",
    "\n",
    "In machine learning, it is always a good idea to play around with different predictive models and their parameters to arrive at the best choice. Fine-tuning your machine learning model is helpful in achieving good results, and of course, cross validation helps you know if you are on the right track to get a good predictive model.\n",
    "\n",
    "\n",
    "_Limitations of Cross Validation_ <br>\n",
    "For cross validation to give some meaningful results, the training set and the validation set are required to be drawn from the same population. Also, human biases need to be controlled, or else cross validation will not be fruitful.\n",
    "\n",
    "_**Other Applications**_\n",
    "\n",
    "_Compare Performance_<br>\n",
    "Suppose you want to make a classifier for the MNIST data set, which consists of hand-written numerals from 0 to 9. You are considering using either K Nearest Neighbours (KNN) or Support Vector Machine (SVM). To compare the performance of the two machine learning models on the given data set, you can use cross validation. This will help you determine which predictive model you should choose working with for the MNIST data set.\n",
    "Cross validation can also be used for selecting suitable parameters. The example mentioned below will illustrate this point well.\n",
    "\n",
    "_Fine-tune Parameters_<br>\n",
    "Suppose you have to build a K Nearest Neighbours (KNN) classifier for the MNIST data set. To use this classifier, you should provide an appropriate value of the parameter k to the classifier. Choosing the value of k intuitively is not a good idea (beware of overfitting!). You can play around with different values of the parameter k and use cross validation to estimate the performance of the predictive model corresponding to each k. You should finally go ahead with the value of k that gives the best performance of the predictive model on the given data set.\n",
    "For the K Nearest Neighbours (KNN) classifier, you can even choose different metrics (default is ‘minkowski’ if you use ‘KNeighborsClassifier’ of sklearn). So you can use cross validation to determine which metric is the best for the data set you have.\n",
    "\n",
    "_References_\n",
    "- https://machinelearningmastery.com/k-fold-cross-validation/\n",
    "- https://towardsdatascience.com/5-reasons-why-you-should-use-cross-validation-in-your-data-science-project-8163311a1e79\n",
    "- https://www.researchgate.net/post/What_is_the_purpose_of_performing_cross-validation\n",
    "- https://medium.com/datadriveninvestor/k-fold-cross-validation-6b8518070833\n",
    "- https://www.cs.tau.ac.il/~nin/Courses/NC05/pr_l13.pdf\n",
    "- https://magoosh.com/data-science/k-fold-cross-validation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
